<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Geometry-Guided Self-Supervised Learning for Category-Level Object Pose Estimation in Robotic Grasping.">
  <meta name="keywords" content="Pose estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Geometry-Guided Self-Supervised Learning for Category-Level Object Pose Estimation in Robotic Grasping</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
<!-- 引入MathJax并配置 -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax.config = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Geometry-Guided Self-Supervised Learning for Category-Level Object Pose Estimation in Robotic Grasping</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Jin Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Kai Sun</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Yanzi Miao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zychaoqun.wixsite.com/chaoqun">Chaoqun Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://irmv.sjtu.edu.cn/">Hesheng Wang</a><sup>3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>China University of Mining and Technology,</span>
            <span class="author-block"><sup>2</sup>Shandong University,</span>
            <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/A6L95qo07fc?si=V3vvSy4ybnM9tupV"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Code</span>-->
<!--                  </a>-->
<!--              </span>-->
              <!-- Dataset Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video id="teaser" autoplay muted loop playsinline height="100%">-->
<!--        <source src="./static/videos/teaser.mp4"-->
<!--                type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into-->
<!--        free-viewpoint-->
<!--        portraits.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-steve">-->
<!--          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/steve.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/chair-tp.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/shiba.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-fullbody">-->
<!--          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/fullbody.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-blueshirt">-->
<!--          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/blueshirt.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-mask">-->
<!--          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/mask.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-coffee">-->
<!--          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/coffee.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-toby">-->
<!--          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/toby2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Accurate 6D pose estimation of diverse objects is critical for the successful execution of robotic manipulation tasks. Among them, category-level 6D object pose and size estimation methods have attracted substantial research attention within precision robotic manipulation domains, demonstrating capability in estimating poses for unseen objects. Nevertheless, most existing methods heavily rely on large-scale annotated datasets or synthetic data, resulting in severely limited generalization performance when confronted with data scarcity or cross-domain distribution differences. To address this limitation, we propose a geometry-guided category-level pose and size estimation based on self-supervised learning, without utilizing real-world label information. Specifically, to enhance the geometric correlations between the negative samples, we incorporate the parameter-inverse transformations into the generation process, facilitating robust consistent learning through inherent structural invariance. Moreover, we incorporate the geometric constraints loss into the network optimization process to further improve the performance of the transformations. Then, we propose the gated geometric feature fusion module and symmetry-aware deformable feature fusion module from the perspective of the geometric feature differences between the object point cloud and the prior point cloud, thereby forming a hierarchical deformable modeling with complementary local-to-global characteristics. Extensive quantitative and qualitative experiments conducted on REAL275 and Wild6D to demonstrate the effectiveness and efficiency of our proposed method. Moreover, we apply the proposed method to the grasping task based on the Aubo robotic arm. The proposed method also achieves remarkable performance in the real-world experiment.
            
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
<iframe width="560" height="315" src="https://www.youtube.com/embed/A6L95qo07fc?si=vP5BlXgE9V7kAL2y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>        </div>
      </div>
    </div>

    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Quantitative Results</h2>
        <p>Quantitative analysis of FLOPs, memory usage, and runtime are indeed the core basis for evaluating the scalability of our method. To address this, we have supplemented detailed computational cost data of our method in comparison with the baseline method MH6D (a key comparative method in the paper), and the results are shown in the table below. The batch size is set to 24.</p>
        <!-- 移除面板的背景和边框 -->
        <div class="columns is-vcentered interpolation-panel" style="background: none; border: none;">
          <div class="column is-full has-text-centered">
            <!-- 移除图片的边框和背景 -->
            <img src="./static/images/table01.jpg" 
                class="interpolation-image" 
                alt="Main archimage" 
                style="border: none; background: none;"/>
          </div>
        </div>

   
        <p>As observed from the table, our method exhibits distinct differences from MH6D in key computational metrics: it outperforms MH6D in terms of FLOPs, CPU/GPU memory usage, and training/inference time. This fully highlights the "lightweight and efficient" design advantage of our method, which serves as the fundamental basis for its strong scalability. Notably, the "training time" and "inference time" in the table only refer to the computation process of the model itself and do not include other links of the robotic grasping system (such as camera data acquisition, robotic arm motion planning, or grasping execution). Additionally, the current training time of $\sim$40 minutes per epoch is constrained by the storage bottleneck of our experimental equipment: all training data is stored on a mechanical hard disk (HDD) rather than a solid-state drive (SSD), resulting in low data loading efficiency. This bottleneck is unrelated to the computational efficiency of the model itself. Even so, the presented computational cost results still sufficiently demonstrate the potential scalability of our method.</p>
         <br/>
        <h2 class="title is-3">Qualitative Results</h2>
        
        <!-- Interpolating. -->
        <h3 class="title is-4">Real-World Scene</h3>
        <div class="content has-text-justified">
          <p>
            We present some additional prediction results of our method in real-world scenarios.
          </p>
        </div>

        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-4 has-text-centered">
              <img src="./static/images/1.png"
                   class="interpolation-image"
                   alt="第一张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/2.png"
                   class="interpolation-image"
                   alt="第二张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/3.png"
                   class="interpolation-image"
                   alt="第三张示例图片"/>
          </div>

      </div>
              <div class="columns is-vcentered interpolation-panel">
          <div class="column is-4 has-text-centered">
              <img src="./static/images/4.png"
                   class="interpolation-image"
                   alt="第一张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/5.png"
                   class="interpolation-image"
                   alt="第二张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/6.png"
                   class="interpolation-image"
                   alt="第三张示例图片"/>
          </div>






      </div>



        <h3 class="title is-4">Results on Wild6D</h3>
        <div class="content has-text-justified">
          <p>
            We present some additional prediction results of our method on Wild6D dataset.
          </p>
        </div>
<div class="columns is-vcentered interpolation-panel">
          <div class="column is-4 has-text-centered">
              <img src="./static/images/11.png"
                   class="interpolation-image"
                   alt="第一张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/12.png"
                   class="interpolation-image"
                   alt="第二张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/13.png"
                   class="interpolation-image"
                   alt="第三张示例图片"/>
          </div>

      </div>
              <div class="columns is-vcentered interpolation-panel">
          <div class="column is-4 has-text-centered">
              <img src="./static/images/14.png"
                   class="interpolation-image"
                   alt="第一张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/15.png"
                   class="interpolation-image"
                   alt="第二张示例图片"/>
          </div>
          <div class="column is-4 has-text-centered">
              <img src="./static/images/16.png"
                   class="interpolation-image"
                   alt="第三张示例图片"/>
          </div>




      </div>
 <br/>
    <h3 class="title is-4">Failure Cases</h3>
    <p>Although our proposed algorithm achieves significant improvements over previous methods, it inevitably exhibits certain failure cases, as illustrated in the figure below with examples from Wild6D and REAL275 datasets.  Red and green boxes denote the results of our method and ground-truth results. </p>
       <div class="columns is-vcentered interpolation-panel" style="background: none; border: none;">
          <div class="column is-full has-text-centered">
            <!-- 移除图片的边框和背景 -->
            <img src="./static/images/faliurecases.jpg" 
                class="interpolation-image" 
                alt="Main archimage" 
                style="border: none; background: none;"/>
          </div>
        </div>
      <p>These failures primarily stem from our reliance on the segmentation results from the pre-trained Mask RCNN object. Therefore, when object localization errors occur (\textit{e.g.,} misidentified object boundaries or incomplete segmentations), subsequent pose estimation becomes unreliable. A potential solution involves integrating large vision foundation models to enhance segmentation precision. By leveraging their superior contextual understanding and fine-grained feature extraction capabilities, we can obtain more accurate object masks, which would in turn improve the robustness of pose estimation under complex occlusion and lighting conditions.</p>

 <br/>
          <h3 class="title is-4">Ablation for residual feature</h3>
    <p> The residual feature $\Delta$F in our method is of shape [B, 128, 1024], where B denotes the batch size. Direct visualization of such vectors is relatively abstract, so we instead adopt a comparative training approach: training the model with and without this residual feature, then visualizing the resulting differences. The corresponding visualization examples are presented in the figure below.</p>
       <div class="columns is-vcentered interpolation-panel" style="background: none; border: none;">
          <div class="column is-full has-text-centered">
            <!-- 移除图片的边框和背景 -->
            <img src="./static/images/ablation.jpg" 
                class="interpolation-image" 
                alt="Main archimage" 
                style="border: none; background: none;"/>
          </div>
        </div>
      <p>As can be observed from the figures, when $\Delta$F is removed, the predicted bounding boxes for detailed boundaries (as indicated by the blue arrow) exhibit noticeable inaccuracies. These results demonstrate that $\Delta$F is critical for bridging the gap between "global pose estimation" and "local boundary refinement," making the model's improvement in pose prediction accuracy more interpretable. </p>

       
        <!--/ Interpolating. -->


      </div>
    </div>
    <!--/ Animation. -->



  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Jin Liu, Kai Sun, Yanzi Miao, Chaoqun Wang, Hesheng Wang},
  title     = {Geometry-Guided Self-Supervised Learning for Category-Level Object Pose Estimation in Robotic Grasping},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a rel="license"
                                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
<!--          <p>-->
<!--            This means you are free to borrow the <a-->
<!--              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,-->
<!--            we just ask that you link back to this page in the footer.-->
<!--            Please remember to remove the analytics code included in the header of the website which-->
<!--            you do not want on your website.-->
<!--          </p>-->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
